{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CFRCChatBot\n",
    "This project develop an end-to-end ChatBot based on Llama2 and RAG on AWS Sagemaker."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bbe454e7b4cef73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Embeddings Deployment\n",
    "This section is to deploy the Embedding model as endpoint."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eda5c5a071e997b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -qU pypdf chromadb==0.4.10 langchain==0.0.295  tiktoken faiss-cpu tqdm\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2537afbfd892890"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from typing import Any, Dict, List, Optional\n",
    "import json\n",
    "import sagemaker, boto3\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = \"cfrcchatbot\"# sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = \"cfrcchatbot\" #sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = (\n",
    "    \"djl-serving\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()\n",
    "\n",
    "\n",
    "!mkdir -p emb_torch\n",
    "%%writefile ./emb_torch/serving.properties\n",
    "engine=Python\n",
    "!pygmentize emb_torch/serving.properties | cat -n\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "486d98a6571303a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile./emb_torch/model.py\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from djl_python import Input, Output\n",
    "\n",
    "JSON_CONTENT_TYPE = 'application/json'\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Load the model for inference\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    model_dict = {'model': model, 'tokenizer': tokenizer}\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    Apply model to the incoming request\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = model['tokenizer']\n",
    "    model = model['model']\n",
    "\n",
    "    encoded_input = tokenizer(input_data, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    model = model_fn()\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "\n",
    "    input_sentences = data[\"inputs\"]\n",
    "\n",
    "    outputs = predict_fn(input_sentences, model)\n",
    "    result = {\"outputs\": outputs.cpu().numpy()}\n",
    "    return Output().add_as_json(result)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db0c4a601eac5dad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "%%writefile ./emb_torch/requirements.txt\n",
    "transformers==4.30.2\n",
    "torch==2.0.1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "916cc12026e5c5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "!rm emb_torch.tar.gz\n",
    "!tar czvf emb_torch.tar.gz -C emb_torch .\n",
    "s3_code_artifact = sess.upload_data(\"emb_torch.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")\n",
    "\n",
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.23.0-cpu-full\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa2f7f20a813f76c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(f\"emb-torch\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact\n",
    "    },\n",
    "\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")\n",
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.t2.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 600,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")\n",
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b134db125427baaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "session = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = session._region_name\n",
    "\n",
    "\n",
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(self, texts: List[str], chunk_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        # return self._embedding_func(texts)\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "        for i in tqdm(range(0, len(texts), _chunk_size)):\n",
    "            response = self._embedding_func(texts[i : i + _chunk_size])\n",
    "            results.extend(response)\n",
    "        return results\n",
    "\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": {**model_kwargs}})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode('utf8'))['outputs']\n",
    "        embeddings = response_json\n",
    "        return embeddings\n",
    "\n",
    "parameters = { 'pooling': 'mean', 'normalize': True}\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "embed_endpoint_name=endpoint_name\n",
    "embeddings = SagemakerEndpointEmbeddingsJumpStart(\n",
    "    endpoint_name=embed_endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e21b12a9d82ca1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. GPT Deployment\n",
    "This section is to deploy the LLM model Llama2 as endpoint."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab2e7ba029f492a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = \"cfrcchatbot\"# sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = \"cfrcchatbot\" #sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = (\n",
    "    \"djl-serving\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "s3_model_prefix = \"model\"\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()\n",
    "!rm -rf Llama2_deepspeed\n",
    "!mkdir -p Llama2_deepspeed\n",
    "\n",
    "%%writefile Llama2_deepspeed/serving.properties\n",
    "engine=DeepSpeed\n",
    "option.tensor_parallel_degree=4\n",
    "option.s3url = {{s3url}}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "915ee80389a4e779"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "template = jinja_env.from_string(Path(\"Llama2_deepspeed/serving.properties\").open().read())\n",
    "Path(\"Llama2_deepspeed/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3url=\"s3://cfrcchatbot/model/Llama-2-13b-chat-hf/\")\n",
    ")\n",
    "!pygmentize Llama2_deepspeed/serving.properties | cat -n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35e57b3a3d53c939"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile ./Llama2_deepspeed/model.py\n",
    "from djl_python import Input, Output\n",
    "import deepspeed\n",
    "import torch\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import cuda, bfloat16\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "\n",
    "    model_location = properties[\"model_dir\"]\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties[\"model_id\"]\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_location,torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "    model = deepspeed.init_inference(model,\n",
    "                                     mp_size=tensor_parallel,\n",
    "                                     dtype=torch.bfloat16,\n",
    "                                     replace_method='auto',\n",
    "                                     replace_with_kernel_inject=True\n",
    "                                    )\n",
    "    class StopOnTokens(StoppingCriteria):\n",
    "        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "            for stop_ids in stop_token_ids:\n",
    "                if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "    device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "    stop_list = ['\\nHuman:', '\\n```\\n', '\\n']\n",
    "    stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "    stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "\n",
    "    local_rank = int(os.getenv('LOCAL_RANK', '0'))\n",
    "    generate_text = pipeline(\n",
    "        model=model, tokenizer=tokenizer,\n",
    "        return_full_text=True, \n",
    "        task='text-generation',\n",
    "        stopping_criteria=stopping_criteria, \n",
    "        temperature=0.3, \n",
    "        max_new_tokens=512, \n",
    "        repetition_penalty=1.2,\n",
    "        device=local_rank\n",
    "    )\n",
    "    return generate_text\n",
    "    return model.module, tokenizer\n",
    "\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "generator = None\n",
    "predictor = None\n",
    "\n",
    "\n",
    "def run_inference(model, tokenizer, data, params):\n",
    "    generate_kwargs = params\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    input_tokens = tokenizer.encode_plus(data,return_token_type_ids=False, truncation= True, max_length = 1024, \n",
    "                                         padding=True, return_tensors=\"pt\").to(torch.cuda.current_device())\n",
    "    outputs = model.generate(**input_tokens, **generate_kwargs)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global predictor\n",
    "    if not predictor:\n",
    "        predictor = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_json()\n",
    "    result = predictor(data[\"inputs\"])\n",
    "    return Output().add_as_json(result)\n",
    "\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "    \n",
    "    input_sentences = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    "    \n",
    "    outputs = run_inference(model, tokenizer, input_sentences, params)\n",
    "    result = {\"outputs\": outputs}\n",
    "    return Output().add_as_json(result)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8234d06f957aa7ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "%%writefile ./Llama2_deepspeed/requirements.txt\n",
    "transformers==4.30.2\n",
    "accelerate==0.20.3\n",
    "xformers\n",
    "deepspeed==0.10.3\n",
    "bitsandbytes==0.41.0\n",
    "einops\n",
    "torch==2.0.1\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cf07ae72b9f2ae8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7490c74a2c48893"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz -C Llama2_deepspeed .\n",
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "515245fefec9cccb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(f\"Llama2-deepspeed\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact\n",
    "    },\n",
    "\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")\n",
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.12xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 300,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")\n",
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a329322ab8d7f9c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "parameters = { \"max_new_tokens\":256,\n",
    "                    \"temperature\":0.2,\n",
    "                    \"repetition_penalty\":1.3,\"length_penalty\":1.2,\n",
    "                    'do_sample': True,}\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": {**model_kwargs}})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        ans = json.loads(output.read().decode('utf8'))['outputs']\n",
    "\n",
    "        return ans#[0]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "endpoint_name=endpoint_name\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9efc5d7855f4865e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Chatbot\n",
    "This section is to download the text data from CFRC Web and deploy the ChatBot."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a3afd1f4d7c52f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download text data from web\n",
    "!pip install -q xmltodict  BeautifulSoup4\n",
    "import xmltodict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    return '\\n'.join(line for line in lines if line)\n",
    "\n",
    "r = requests.get(\"https://cityfutures.ada.unsw.edu.au/sitemap.xml\")\n",
    "xml = r.text\n",
    "raw = xmltodict.parse(xml)\n",
    "\n",
    "pages = []\n",
    "text = ''\n",
    "c=0\n",
    "for info in raw['urlset']['url']:\n",
    "    url = info['loc']\n",
    "    if 'https://cityfutures.ada.unsw.edu.au/' in url:\n",
    "        pages.append({'text': extract_text_from(url), 'source': url})\n",
    "        text = extract_text_from(url)       \n",
    "        \n",
    "        text_file = open(\"doc/CFRC_web_page\"+str(c)+\".txt\", \"w\")\n",
    "        n = text_file.write(text)\n",
    "        text_file.close()\n",
    "        c+=1\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b99f04924aec8321"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# deploy the chatbot\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader,TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "loader = TextLoader(\"CFRC_web.txt\")\n",
    "loader = DirectoryLoader(\"doc\", glob='**/*.txt', show_progress=True, loader_cls=TextLoader)\n",
    "documents_text = loader.load_and_split()\n",
    "\n",
    "loader = PyPDFLoader(\"brochure.pdf\")\n",
    "documents_pdf = loader.load_and_split()\n",
    "text_splitter = TokenTextSplitter(chunk_size=300, chunk_overlap=10)\n",
    "texts = text_splitter.split_documents(documents_text+documents_pdf)\n",
    "docsearch = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "def CFRCbot(q):\n",
    "    \n",
    "    docs = docsearch.similarity_search(question, k=2)\n",
    "    prompt_template = \"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\\n\\n\"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    chain = load_qa_chain(llm=sm_llm, prompt=PROMPT)\n",
    "\n",
    "    result = chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)[\n",
    "        \"output_text\"\n",
    "    ]\n",
    "    return result.split(\"\\n\\n\")[-1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9682a6bca0b4476"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "question = \"What is 2+2?\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))\n",
    "\n",
    "question = \"What did Professor Chris Pettit do?\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))\n",
    "\n",
    "question =  \"Who is Chris Pettit\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))\n",
    "\n",
    "question =  \"Who is Yang Lin at UNSW CFRC\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))\n",
    "\n",
    "question =  \"I am doing research in Planning of future cities, how do I collaborate with UNSW CFRC\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))\n",
    "\n",
    "question =  \"What did city futures research center do\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))\n",
    "\n",
    "question =  \"Can you list some research topics of city futures research center do\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))\n",
    "\n",
    "question =  \"How can I reach out to Chris Pettit\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))\n",
    "\n",
    "question = \"Is Yang Lin handsome\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))\n",
    "\n",
    "question = \"Is Yang Lin ethical\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))\n",
    "\n",
    "question = \"Is Yang Lin a good cooker\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))\n",
    "\n",
    "question = \"Is Yang Lin a good researcher\"\n",
    "print(\"\\nQuestion: \"+question+\"\\n\"+CFRCbot(question))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5509748191d89036"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
